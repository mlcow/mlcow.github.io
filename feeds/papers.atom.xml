<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mlcow - papers</title><link href="https://mlcow.github.io/" rel="alternate"></link><link href="https://mlcow.github.io/feeds/papers.atom.xml" rel="self"></link><id>https://mlcow.github.io/</id><updated>2019-10-19T00:00:00+05:30</updated><entry><title>Batch Normalization</title><link href="https://mlcow.github.io/batch-normalization.html" rel="alternate"></link><published>2019-10-19T00:00:00+05:30</published><updated>2019-10-19T00:00:00+05:30</updated><author><name>voo4</name></author><id>tag:mlcow.github.io,2019-10-19:/batch-normalization.html</id><summary type="html">&lt;p&gt;Summary on &lt;a href='#DBLP:journals/corr/IoffeS15' id='ref-DBLP:journals/corr/IoffeS15-1'&gt;Ioffe and Szegedy (2015)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Internal Covariate Shift&lt;/strong&gt; is defined as &lt;em&gt;the change in the distribution of network activations due to the change in network parameters during training.&lt;/em&gt; It captures the change in activation values as training progresses. In a Deep Neural Network this change in distribution of inputs â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Summary on &lt;a href='#DBLP:journals/corr/IoffeS15' id='ref-DBLP:journals/corr/IoffeS15-1'&gt;Ioffe and Szegedy (2015)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Internal Covariate Shift&lt;/strong&gt; is defined as &lt;em&gt;the change in the distribution of network activations due to the change in network parameters during training.&lt;/em&gt; It captures the change in activation values as training progresses. In a Deep Neural Network this change in distribution of inputs at each layer, as training progresses demands - carefuly paremeter initailization and slower learning rates. Batch Normalization is a technique that helps address these problems arising from internal covariate shift. &lt;/p&gt;
&lt;p&gt;Also batch normalizaiton helps by
- Permitting higher learning rates, and less careful initilization
- Acting as Regularizer, eliminating the need for Dropout
- Faster training. (Converges with 14 times fewer steps in the examples mentioned in the article)&lt;/p&gt;
&lt;h2&gt;The problem&lt;/h2&gt;
&lt;p&gt;Training a Deep Neural Network is complicated - As outputs and gradients are affected by previous layers. Even careful parameter initialization can help keep inputs to activations in good regions, only to a limited extent/time. Beyond which gradients are zero either due to saturation or activations close to origin.&lt;/p&gt;
&lt;p&gt;Problems arising from Internal Covariate Shift:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When activations move out of linear regime of sigmoid, Gradients stop flowing.&lt;/li&gt;
&lt;li&gt;And when using a relu, gradients are highly dependent on activation values (which experience an internal covariance shift). This might result in uncontrolled step sizes. Thus slower learning rates are required for convergence.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These problems are usually addressed using&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RELU as activation&lt;/li&gt;
&lt;li&gt;Careful initialization of weights (&lt;a href='#glorot2010understanding' id='ref-glorot2010understanding-1'&gt;Glorot and Bengio (2010)&lt;/a&gt;, &lt;a href='#DBLP:journals/corr/SaxeMG13' id='ref-DBLP:journals/corr/SaxeMG13-1'&gt;Saxe et al. (2014)&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Small learning rates&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Batch Normalization&lt;/h2&gt;
&lt;p&gt;Network training is known to converge faster if its inputs are whitened &lt;a href='#lecun1998efficient' id='ref-lecun1998efficient-1'&gt;LeCun et al. (1998)&lt;/a&gt; - i.e., linearly transformed to have zero means and unit variances, and decorrelated.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is know that convergence is usually faster if the average of each input is close to zero. In an extreme of all inputs being positive (or all inputs being negative) the weight updates have to be all of the same sign. The weights can only all decrease or all increase. If the actual solution requires some to decrease and some to increase, it can only be done by zigzagging. See &lt;a href="https://www.youtube.com/watch?v=wEoyxE0GP2M&amp;amp;feature=youtu.be&amp;amp;t=526"&gt;Youtube: Lecture 6 | Training Neural Networks I | CS231n&lt;/a&gt; for a detailed explanation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Whiteninig the inputs to a neural network [Mean canellation + Decorrelation(PCA) + Covariance equalization] helps in better gradient flow. But the effects are diminished as we go deeper in the network. Batch normalization tries to apply the same at every layer, in an attempt to removing ill effects of the internal covariate shift.&lt;/p&gt;
&lt;p&gt;However a full whitening would require computation of Covariance matrix &lt;span class="math"&gt;\(\text{Cov}[x] = \mathbb{E}_{x \in \mathcal{X}}[xx^T] - \mathbb{E}[x]\mathbb{E}[x]^T\)&lt;/span&gt; and its inverse square root for computation of whitened activations. This is computationally complex and adds more complexity to the backpropogation. Hence decorrelation is typically skipped.&lt;/p&gt;
&lt;h4&gt;How it's done&lt;/h4&gt;
&lt;h5&gt;At train time&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;Each dimension's mean and variance is computed across the samples in the mini batch. The input is mean cancelled and scaled by inverse of variance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For a layer with &lt;span class="math"&gt;\(d\)&lt;/span&gt;-dimensional input &lt;span class="math"&gt;\(\mathrm{x}=\left(x^{(1)} \ldots x^{(d)}\right)\)&lt;/span&gt;, each dimension is normalized
&lt;/p&gt;
&lt;div class="math"&gt;$$\widehat{x}^{(k)}=\frac{x^{(k)}-\mathrm{E}\left[x^{(k)}\right]}{\sqrt{\operatorname{Var}\left[x^{(k)}\right] + \epsilon}}$$&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;The above normalization would result in constraining the inputs of every layer to &lt;span class="math"&gt;\([-1, 1]\)&lt;/span&gt; (a soft constraint). This will make it impossible for the activations (say sigmoid) to ever learn to saturate. Hence additional provision is made to enable the normalization to become an identity function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For each activation &lt;span class="math"&gt;\(x^{(k)}\)&lt;/span&gt; variables &lt;span class="math"&gt;\(\gamma^{(k)}, \beta^{(k)}\)&lt;/span&gt; are introduced which scale and shift the normalized value
&lt;/p&gt;
&lt;div class="math"&gt;$$
y^{(k)}=\gamma^{(k)} \widehat{x}^{(k)}+\beta^{(k)}
$$&lt;/div&gt;
&lt;p&gt;This &lt;strong&gt;Batch Normalizing Transformation&lt;/strong&gt; can be represented as:
&lt;/p&gt;
&lt;div class="math"&gt;$$ \mathrm{BN}_{\gamma, \beta}: x_{1 \ldots m} \rightarrow y_{1 \ldots m} $$&lt;/div&gt;
&lt;h5&gt;At evaluation time&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;During evaluation/test, population statistics replace the batch statistics in batch normalization transformation&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="math"&gt;$$
\widehat{x}=\frac{x-\mathrm{E}[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}
$$&lt;/div&gt;
&lt;p&gt;At every layer, for the estimates of &lt;span class="math"&gt;\(\mathrm{E}\left[x\right]\)&lt;/span&gt; and &lt;span class="math"&gt;\(\operatorname{Var}\left[x\right]\)&lt;/span&gt;, population statistics are used. &lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned} \mathrm{E}[x] &amp;amp; = \mathrm{E}_{\mathcal{B}}\left[\mu_{\mathcal{B}}\right] \\ \operatorname{Var}[x] &amp;amp; = \frac{m}{m-1} \mathrm{E}_{\mathcal{B}}\left[\sigma_{\mathcal{B}}^{2}\right] \text{ An unbiased estimate for variance, considering } m \text{ batches} \end{aligned}
$$&lt;/div&gt;
&lt;h4&gt;With batch normalization, the gradients flow through additional paths&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Batch Normailzation is a differentiable transformation that introduces normalized activations into the network.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The mean, variance, etc., computed during normalization are considered for gradient backpropagation.&lt;/p&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(x\)&lt;/span&gt; be an input to a layer &lt;span class="math"&gt;\(F(x,...)\)&lt;/span&gt;. And &lt;span class="math"&gt;\(\mathcal{X} = x_{1..N}\)&lt;/span&gt; is the set of values of &lt;span class="math"&gt;\(x\)&lt;/span&gt; over the training set.&lt;/p&gt;
&lt;p&gt;With batch normalization &lt;span class="math"&gt;\(F(x,...)\)&lt;/span&gt; is transformed to &lt;span class="math"&gt;\(F(\widehat{x},...)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\widehat{x} = \text{Norm}(x, \mathcal{X})\)&lt;/span&gt; represents the whitening/normalization process.&lt;/p&gt;
&lt;p&gt;In unnormalized graph backprop, gradients flowing to &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; wouldn't flow through rest of &lt;span class="math"&gt;\(\mathcal{X}\)&lt;/span&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial L}{\partial x_i} = \frac{\partial F}{\partial x_i}\frac{\partial L}{\partial F}  $$&lt;/div&gt;
&lt;p&gt;In the normalized version backprop, &lt;span class="math"&gt;\(F(\widehat{x},...)\)&lt;/span&gt; gradients through &lt;span class="math"&gt;\(\widehat{x}_i\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial L}{\partial \widehat{x}_i} = \frac{\partial F}{\partial \widehat{x}_i}\frac{\partial L}{\partial F}  $$&lt;/div&gt;
&lt;p&gt;
further flow through &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; and rest of &lt;span class="math"&gt;\(\mathcal{X}\)&lt;/span&gt;, we need to calculate:
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial \text{Norm}(x_i,\mathcal{X})}{\partial x_i} \text{ and jacobian }  \frac{\partial \text{Norm}(x_i,\mathcal{X})}{\partial \mathcal{X}} $$&lt;/div&gt;
&lt;h3&gt;Benefits&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Activations can receive inputs in the linear regime throught all layers.&lt;/li&gt;
&lt;li&gt;Beneficial on the gradient flow through the network, by reducing the dependence of gradients on the scale of parameters. Thus higher learning rates can be used.&lt;/li&gt;
&lt;li&gt;Regularizes the model, reducing the need for Dropout.&lt;blockquote&gt;
&lt;p&gt;A training example is seen in conjunction with the other examples in the mini-batch. Thus the network cannot produce deterministic values for &lt;em&gt;a&lt;/em&gt; given training example. Thus acting like a regularizer. In other words, if the network tries to overfit a given example, it inadvertantly effects other training examples and thus will be penalized with a higher loss.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Tensorflow implementation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;TODO&lt;/strong&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;hr/&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;p class='cite-p' id='glorot2010understanding'&gt;Xavier Glorot and Yoshua Bengio.
Understanding the difficulty of training deep feedforward neural networks.
In &lt;em&gt;Proceedings of the thirteenth international conference on artificial intelligence and statistics&lt;/em&gt;, 249â€“256. 2010.
URL: &lt;a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"&gt;http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-glorot2010understanding-1" title="Jump back to reference 1"&gt;â†©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p class='cite-p' id='DBLP:journals/corr/IoffeS15'&gt;Sergey Ioffe and Christian Szegedy.
Batch normalization: accelerating deep network training by reducing internal covariate shift.
&lt;em&gt;CoRR&lt;/em&gt;, 2015.
URL: &lt;a href="http://arxiv.org/abs/1502.03167"&gt;http://arxiv.org/abs/1502.03167&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1502.03167"&gt;arXiv:1502.03167&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-DBLP:journals/corr/IoffeS15-1" title="Jump back to reference 1"&gt;â†©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p class='cite-p' id='lecun1998efficient'&gt;Yann LeCun, Leon Bottou, G&amp;nbsp;Orr, and Klaus-Robert Muller.
Efficient backprop.
&lt;em&gt;Neural Networks: Tricks of the Trade. New York: Springer&lt;/em&gt;, 1998.
URL: &lt;a href="http://cseweb.ucsd.edu/classes/wi08/cse253/Handouts/lecun-98b.pdf"&gt;http://cseweb.ucsd.edu/classes/wi08/cse253/Handouts/lecun-98b.pdf&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-lecun1998efficient-1" title="Jump back to reference 1"&gt;â†©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p class='cite-p' id='DBLP:journals/corr/SaxeMG13'&gt;Andrew&amp;nbsp;M. Saxe, James&amp;nbsp;L. McClelland, and Surya Ganguli.
Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.
In &lt;em&gt;2nd International Conference on Learning Representations, &lt;span class="bibtex-protected"&gt;ICLR&lt;/span&gt; 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings&lt;/em&gt;. 2014.
URL: &lt;a href="http://arxiv.org/abs/1312.6120"&gt;http://arxiv.org/abs/1312.6120&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-DBLP:journals/corr/SaxeMG13-1" title="Jump back to reference 1"&gt;â†©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;</content></entry></feed>