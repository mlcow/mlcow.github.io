<!DOCTYPE html>
<html lang="en">
<head>
        <title>Batch Normalization</title>
        <meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link rel="shortcut icon" href="https://mlcow.github.io/theme/images/favicon.ico"/>
        <link rel="stylesheet" href="https://mlcow.github.io/theme/css/main.css" type="text/css" />
        <link href="https://mlcow.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="mlcow Atom Feed" />

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="https://mlcow.github.io/css/ie.css"/>
                <script src="https://mlcow.github.io/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="https://mlcow.github.io/css/ie6.css"/><![endif]-->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js" type="text/javascript"></script>


</head>

<body id="index" class="home">
	
  <!-- <header id="banner" class="body"> -->
  <!--               <h1><a href="https://mlcow.github.io/"><img src="http://www.launchyard.com/images/logo.png" /><strong>mlcow</strong></a></h1> -->
  <!--       </header> --> 

  <div class="LaunchyardDetail">
    <!-- <p> -->
    <!-- <img src="https://mlcow.github.io/theme/images/blue-pin.png" width="100" height="100" alt="Graph icon"> -->
    <!-- </p> -->
    <p><a id="sitesubtitle" href="https://mlcow.github.io/">mlcow</a></p>
    <!-- <p><a id="aboutlink" href="https://mlcow.github.io/pages/about.html">about</a></p> -->

    <ul> 
      <li><a href="https://mlcow.github.io/pages/about.html">about</a></li> 
      <!-- <li><a href="https://mlcow.github.io/pages/about.html">articles</a></li> -->
<!--  -->
<!--         <br/> -->
<!--         <br/> -->
<!--      -->        
<!--       <li><a href="">Good reads</a></li> -->
<!--      -->        
<!--       <li><a href="">First Moo!</a></li> -->
<!--      -->
<!--  -->
    </ul> 

         <hr/> 
         <br/> 

    <!-- This belongs in the about page -->
      <!--  -->
      <!-- <a href="#">You can add links in your config file</a> -->
      <!-- <br/> -->
      <!--  -->
      <!-- <a href="#">Another social link</a> -->
      <!-- <br/> -->
      <!--  -->

<ul>
	<li><a href="https://mlcow.github.io/category/misc.html">misc</a></li>
</ul>
 
  </div>

<section id="content" >
    <div class="body">
      <article>
        <header>
          <h1 class="entry-title">
            <a href="https://mlcow.github.io/drafts/batch-normalization.html" rel="bookmark"
               title="Permalink to Batch Normalization">Batch Normalization</a></h1>

        </header>

        <div class="entry-content">
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="https://mlcow.github.io/author/voo4.html">voo4</a>
        </li>
        <li class="published" title="2019-10-16T00:00:00+05:30">
          on&nbsp;Wed 16 October 2019
        </li>

	</ul>

</div><!-- /.post-info -->          <h1>Batch Normalization</h1>
<p>Summary on <a href='#DBLP:journals/corr/IoffeS15' id='ref-DBLP:journals/corr/IoffeS15-1'>Ioffe and Szegedy (2015)</a></p>
<h2>Introduction</h2>
<ul>
<li>14 times fewer training steps</li>
</ul>
<h2>The problem</h2>
<ul>
<li>Training is complicated due to multiple layers. Outputs and gradients are affected by multiple layers, harder to control.</li>
<li>Internal covariate shift: Change in the distribution of layers' inputs</li>
<li>Problems:</li>
<li>Saturation of sigmoids</li>
<li>Even with a  relu, gradients are highly dependent on output values at each layer. Which might result in haphazard steps with internal covariate shift. Thus slower learning rates are required to avoid missing minima.</li>
</ul>
<h2>Existing attempts</h2>
<ul>
<li>Relu</li>
<li>Careful initialization</li>
<li>Small learning rates</li>
</ul>
<h2>Batch Normalization</h2>
<ul>
<li>Attempts to reduce internal covariate shift.</li>
<li>Also beneficial on the gradient flow through the network, by reducing the dependence of gradients on the scale of parameters.</li>
<li>Thus can have much higher learning rates</li>
<li>
<p>Also happens to regularize the model</p>
</li>
<li>
<p>Internal Covariate Shift: Defined  as <em>the change in the distribution of network activations due to  the  change in network parameters during training.</em></p>
</li>
<li>
<p><a href='#series/lncs/LeCunBOM12' id='ref-series/lncs/LeCunBOM12-1'>LeCun et al. (2012)</a> Network training converges faster if its inputs are whitened - i.e., linearly transformed to have zero means and unit variances, and decorrelated.</p>
</li>
<li>Here we try to whiten inputs at each layer, to get the inputs closer to a fixed distribution, removing ill effects of the internal covariate shift</li>
</ul>
<h4>When normalizing, the gradients at a layer out of each test case flow through additional paths</h4>
<p>Let  <span class="math">\(\mathcal{X} = x_{1..N}\)</span> is the set of values of <span class="math">\(x\)</span> an input to a layer <span class="math">\(F(x,...)\)</span> over the training set.</p>
<p>When normalizing <span class="math">\(F(x,...)\)</span> becomes <span class="math">\(F(\hat{x},...)\)</span>, where <span class="math">\(\hat{x} = \text{Norm}(x, \mathcal{X})\)</span> represents the whitening/normalization process.</p>
<p>In unnormalized graph backprop, gradients flowing to <span class="math">\(x_i\)</span> would only be applicable to the <span class="math">\(i^{th}\)</span> training example's graph:
</p>
<div class="math">$$\frac{\partial L}{\partial x_i} = \frac{\partial F}{\partial x_i}\frac{\partial L}{\partial F}  $$</div>
<p>In the normalized version backprop, <span class="math">\(F(\hat{x},...)\)</span> gradients through <span class="math">\(\hat{x_i}\)</span>
</p>
<div class="math">$$\frac{\partial L}{\partial \hat{x_i}} = \frac{\partial F}{\partial \hat{x_i}}\frac{\partial L}{\partial F}  $$</div>
<p>
further flow through <span class="math">\(x_i\)</span> and entire <span class="math">\(\mathcal{X}\)</span>, we need to calculate:
</p>
<div class="math">$$\frac{\partial \text{Norm}(x_i,\mathcal{X})}{\partial x_i} \text{ and jacobian }  \frac{\partial \text{Norm}(x_i,\mathcal{X})}{\partial \mathcal{X}} $$</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%&#64;#$&#64;#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%&#64;#$&#64;#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script><hr/>
<h2>References</h2>
<ul><li><p class='cite-p' id='DBLP:journals/corr/IoffeS15'>Sergey Ioffe and Christian Szegedy.
Batch normalization: accelerating deep network training by reducing internal covariate shift.
<em>CoRR</em>, 2015.
URL: <a href="http://arxiv.org/abs/1502.03167">http://arxiv.org/abs/1502.03167</a>, <a href="https://arxiv.org/abs/1502.03167">arXiv:1502.03167</a>. <a class="cite-backref" href="#ref-DBLP:journals/corr/IoffeS15-1" title="Jump back to reference 1">↩</a></p></li>
<li><p class='cite-p' id='series/lncs/LeCunBOM12'>Yann LeCun, Léon Bottou, Genevieve&nbsp;B. Orr, and Klaus-Robert Müller.
Efficient backprop.
In Grégoire Montavon, Genevieve&nbsp;B. Orr, and Klaus-Robert Müller, editors, <em>Neural Networks: Tricks of the Trade (2nd ed.)</em>, volume 7700 of Lecture Notes in Computer Science, pages 9&ndash;48.
Springer, 2012.
URL: <a href="http://dblp.uni-trier.de/db/series/lncs/lncs7700.html#LeCunBOM12">http://dblp.uni-trier.de/db/series/lncs/lncs7700.html#LeCunBOM12</a>. <a class="cite-backref" href="#ref-series/lncs/LeCunBOM12-1" title="Jump back to reference 1">↩</a></p></li>
</ul>
        </div><!-- /.entry-content -->
        <div class="comments">

          <div id="disqus_thread"></div>
          <script type="text/javascript">
            var disqus_identifier = "drafts/batch-normalization.html";
            (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = 'https://mlcow-github-io.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
          </script>
        </div>

      </article>
    </div>
</section>
        <section id="extras" >
        
        </section><!-- /#extras -->
	
        <footer id="contentinfo" >
                <address id="about" class="vcard ">
                Proudly powered by <a href="http://getpelican.com/" target="_blank">Pelican</a>, which takes
                great advantage of <a href="http://python.org" target="_blank">Python</a>.
		
                </address><!-- /#about -->
		

                
        </footer><!-- /#contentinfo -->

<script type="text/javascript">
    var disqus_shortname = 'mlcow-github-io';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>
</html>